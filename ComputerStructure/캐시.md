컴퓨터 성능의 가장 중요한 요소 -> CPU 처리 속도 -> 기억 장치 액세스 시간을 낮춰야함 -> cpu와 주기억 장치 사이 고속 소용량 메모리, 캐시

CPU와 캐시 메모리는 워드를 전송, 캐시와 주기억 장치는 블록 전송

CPU에서 기억 장치 접근 시에 먼저 캐시에 해당 정보가 있는지 확인 -> 있으면(hit) 읽고 없다면(miss) 해당 정보를 주기억 장치에서 캐시로 적재 후 읽어들임 -> 반복하다보면 주기억 장치에 있는 정보들이 캐시로 많이 적재되어 특정 시점부터는 성능 향상
H(히트율) = (히트된 횟수) / (전체 기억 장치 액세스 횟수), 1-H(미스율)

참조 지역성 : 많은 프로그램을 분석해보면 한정된 영역에서만 메모리 참조가 이루어짐

공간적 지연성 : 서로 인접한 데이터들끼리 연속적으로 액세스될 확률이 높음
시간적 지연성 : 최근 액세스한 명령어나 데이터는 다시 참조할 확률이 높음

캐시의 목표
    1. 히트율 극대화
    2. 히트한 경우 읽어오는 시간 최소화
    3. 미스인 경우 주기억 장치에서 가져오는 시간 최소화
    4. 캐시 내용 변경 시 주기억 장치에서의 갱신 시간 최소화
캐시 용량이 커진다고 좋은 것은 아님 -> 회로 복잡성 증가로 액세스 시간 증가

블록 : 주기억 장치와 캐시 사이의 정보 단위
k개의 워드로 이루어진 블록에 주기억 장치는 워드 2^n개로 구성되어 각 워드가 n비트의 주소로 지정된다고하면, 전체 블록의 수는 2^n/k 개가 된다. 라인 m개로 구성, k개의 워드가 각 라인에 저장. 라인에는 어떤 블록이 적재되어있는지 알기 위해 태그 저장

사상 방식 : 블록이 어느 캐시 라인에 들어갈지 결정하는 방식
    1. 직접 사상 방식 : 블록을 하나의 캐시 라인에만
    2. 완전-연관 사상 방식 : 어떤 캐시 라인에도 블록 위치시킬 수 있음, 충돌 문제 회피, 공간 효율도 낮음, 검색 속도가 느림
    3. 세트-연관 사상 방식 : 직접 사상과 완전-연관 사상을 절충, 라인을 세트로 나누어 블록을 세트 하나에만 위치시킴, 세트가 작을 시 충돌 발생 가능

교체 알고리즘 : 캐시가 다 채워지면 새로운 블록을 기존의 블록 중 하나에서 교체해야됨.
    LRU : 사용되지 않은 채 오래된 캐시에 적재된 블록을 교체. 각 라인에 USE 비트를 사용하여 최근 액세스된 세트의 비트를 1로하고 비트가 0인 라인을 교체함.
    FIFO : 사용여부와 상관없이 가장 오래된 캐시를 교체한다.
    LFU : 액세스 횟수가 가장 적은 블록을 교체한다. 카운터를 설치
    Radnom : 캐시 라인 중 임의로 선택하여 교체. LFU보다 약간 성능이 떨어짐

쓰기 정책 : 블록을 교체하기 전에 블록 내용이 변경되었는지, 그리고 주기억 장치에도 반영이 되었는지 확인해야된다.
변경이 없다면 그대로 교체하고 변경되었다면 주기억 장치에도 똑같이 갱신을 해야된다.
    write-through : 캐시와 주기억 장치에 동시에 쓴다. 시간이 오래걸림. 함께 쓰기
    write-back : 캐시에서 내용이 변경되어도 update비트가 세트되어 나중에 블록이 교체할 때에만 주기억 장치를 갱신한다.
    캐시에서만 쓰기를 하므로 쓰기 시간이 짧아지고 주기억 장치에 쓰는 행위는 최소화된다. 주기억 장치 일부분이 무효상태가됨. 나중에 쓰기

쓰기 시에 고려 사항
    - 캐시의 워드가 변경되면 해당 주기억 장치의 블록은 무효화
    - 입출력 장치가 주기억 장치의 블록을 변경해도 캐시 블록 무효화
    - 멀티프로세서 시스템에서는 다수 프로세서가 동일 버스에 연결되어 각각 지역 캐시를 가지고 있을 때, 한 쪽에서 워드 변경 시 다른 캐시들은 무효화됨. 이는 데이터 불일치 문제가 발생.


